pacman::p_load(tidyverse, rio, ggplot2, lubridate, quanteda, newsmap)
setwd("C:/Users/murrn/GitHub/nonviolent-repression")
# load data
kprf = read_csv('C:/Users/murrn/GitHub/placeholder_data/kprf_output.csv',
quote = "\'")
activatica = read_csv('C:/Users/murrn/GitHub/placeholder_data/activatica_output.csv',
quote = "\'")
kavkaz = read_csv("C:/Users/murrn/GitHub/placeholder_data//kavkaz_output_complete.csv",
quote = "\'")
kommersant = read_csv("C:/Users/murrn/GitHub/placeholder_data//kommersant_output_complete.csv",
quote = "\'")
#### harmonise dates and create month-year indicator ####
## Helper function and mapping from Yana's code
months_map = c(
января = "January",
февраля = "February",
марта = "March",
апреля = "April",
мая = "May",
июня = "June",
июля = "July",
августа = "August",
сентября = "September",
октября = "October",
ноября = "November",
декабря = "December"
)
replace_months = function(date_string, months_map) {
for (month_rus in names(months_map)) {
date_string <- gsub(month_rus, months_map[month_rus], date_string, fixed = TRUE)
}
return(date_string)
}
# Harmonise data for each source
kprf <- kprf %>%
mutate(date = sub(" \\(.*", "", date_published),
date = ymd_hm(date),
month_year = format(date, "%Y-%m"),
source = "KPRF")
kommersant <- kommersant %>%
mutate(date = sub(" \\(.*", "", date_published),
date = dmy_hm(date),
month_year = format(date, "%Y-%m"),
source = "Kommersant")
activatica <- activatica %>%
mutate(month_year = format(date_created, "%Y-%m"),
source = "Activatica",
content = text)
kavkaz <- kavkaz %>%
mutate(date = replace_months(date_published, months_map),
date = dmy_hm(date),
month_year = format(date, "%Y-%m"),
source = "Kavkaz")
# bind data
all_media = bind_rows(kprf, kommersant, activatica, kavkaz)
# temp: until parsing is fixed: subset with no duplicates
all_media_subset <- all_media %>%
distinct(content, .keep_all = TRUE) %>%
mutate(doc_id = paste0("doc_", row_number()))
corp_media = corpus(all_media_subset, text_field = "content",
docid_field = "doc_id")
#dictionary
dict <- quanteda::dictionary(list(
protest = c('протест'),
rally = c('митинг'),
demonstration = c('демонстрация'),
revolt = c('бунт'),
manifestation = c('манифестация'),
boycott = c('бойкот'),
strike = c('забастовка'),
picketing = c('пикетирование'),
picket = c('пикет'),
walkout = c('стачка')
))
# tokens with no pre-processing
media_toks = tokens(corp_media,  remove_punct = TRUE, remove_number = TRUE) %>%
tokens_remove(pattern = c(stopwords("ru"))) %>%
tokens_wordstem()
dict_toks = tokens_lookup(media_toks, dictionary = dict)
res = convert(dfm(dict_toks), to = "data.frame")
media_res = all_media_subset %>% left_join(res, by = "doc_id")
# create protest story indicator
media_res <- media_res %>%
mutate(protest_indicator = as.integer(protest > 0 | rally > 0 |
demonstration > 0 | protest > 0 |
revolt > 0 | manifestation > 0 |
boycott > 0 | strike > 0 |
picketing > 0 | picket > 0 | walkout > 0))
# Creating a vector of variable names
variables_vector <- c("protest", "rally", "demonstration",
"revolt", "manifestation", "boycott", "strike",
"picketing", "picket", "walkout")
## Count and ratio of protest stories for each source
protest_stats_source <- media_res %>%
group_by(source) %>%
summarise(
total_stories = n(),
protest_stories = sum(protest_indicator),
ratio_protest = protest_stories / total_stories
)
protest_stats_source
protest_stats_source
## How many mentions of a keyword in corpus?
# Calculate the sum of each variable
sums <- sapply(media_res[variables_vector], sum, na.rm = TRUE)
# Print the sums
print(sums)
toks_label <- tokens_lookup(media_toks, dictionary = data_dictionary_newsmap_ru,
levels = 3)
dfmat_label <- dfm(toks_label, tolower = FALSE)
dfmat_label
toks_label
media_toks
media_toks[1]
dfmat_label
toks_label[1:10]
media_toks[1]
View(media_toks[1])
media_toks[1][["doc_1"]]
media_toks[2][["doc_1"]]
View(media_toks[2])
media_toks[2][["doc_2"]]
dfmat_label <- dfm(toks_label, tolower = FALSE)
dfmat_feat <- dfm(media_toks, tolower = FALSE)
dfmat_feat_select <- dfm_select(dfmat_feat, pattern = "^[A-Z][A-Za-z0-9]+",
valuetype = "regex", case_insensitive = FALSE) %>%
dfm_trim(min_termfreq = 10)
tmod_nm <- textmodel_newsmap(dfmat_feat_select, y = dfmat_label)
pred_nm <- predict(tmod_nm)
coef(tmod_nm, n = 15)[c("RU", "US", "GB")]
### number of articles-per country
count <- sort(table(factor(pred_nm, levels = colnames(dfmat_label))), decreasing = TRUE)
head(count, 20)
media_res$newsmap_label <- pred_nm
# print individual stories
media_res$content[media_res$doc_id == "doc_9352"]
media_res$newsmap_label[media_res$doc_id == "doc_9352"]
min(activatica$date_created)
max(activatica$date_created)
activatica <- activatica %>%
mutate(month_year = format(date_created, "%Y-%m"),
source = "Activatica",
content = text,
year = year(date_created))
max(activatica$year)
activatica %>% group_by(year) %>% summarise(n())
unique(media_res$source)
media_res %>% select(source == "Activatica")
media_res %>% select(source = "Activatica")
media_res %>% filter(source == "Activatica")
media_res %>% filter(source == "Activatica") %>% group_by(year, protest_indicator) %>% summarise(n())
media_res %>% filter(source == "Activatica") mutate(year = year(date_created))%>%
media_res %>% filter(source == "Activatica") %>%  mutate(year = year(date_created)) %>%
group_by(year, protest_indicator) %>% summarise(n())
subst = media_res %>% filter(source == "Activatica") %>%  mutate(year = year(date_created)) %>%
group_by(year, protest_indicator) %>% summarise(n())
View(subst)
